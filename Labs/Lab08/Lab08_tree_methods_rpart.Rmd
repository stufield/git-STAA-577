---
title: 'STAA 577: Laboratory Eight </br> `tidyverse` version'
author: 'Adapted by Tavener & Field </br> From: James, Witten, Hastie and Tibshirani'
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_notebook: 
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
ratio: '9:16'
tables: yes
fontsize: 12pt
editor_options: 
  chunk_output_type: inline
---


# Setup

```{r setup, message = FALSE, warning = FALSE}
options(warnPartialMatchArgs = FALSE)  # don't want these warnings
library(magrittr)     # pipes
library(tibble)       # tibbles
library(dplyr)        # data wrangling
library(purrr)        # iteration
library(yardstick)    # classification performance measures
library(broom)        # summarizing models consistently
library(ggplot2)      # tidy plotting
library(ISLR)         # The `Carseats` data set
library(rpart)        # building classification trees
library(randomForest) # package name says it
library(MASS)         # The `Boston` data set
library(gbm)          # Boosting algorithms
library(caret)        # classification and Regresstion Training (CV)
library(gridExtra)    # arranging ggplot `grobs` into grid
```



-----------------------


# The `Carseats` Data Set
```{r Carseats}
Carseats %<>% tibble::as.tibble()
Carseats
dim(Carseats)
```






# Fitting Classification Trees
**NOTE**: In this lab we will use the `rpart::rpart` function instead of the
`tree::tree` function which the ISLR textbook uses. The reasons boil down to:

  1. `rpart` uses "recursive partitioning" to build its trees, which in theory
  should be faster.
  2. Previous experience has shown a *slight* performance improvement in 
  classification problems (though performance is similar in *this* data problem.
  3. It is an implementation of most of the tree fitting algorithm described
  in the 1984 book by Breiman, Friedman, Olshen and Stone (well known).
  4. In the years since ISLR first came out, `rpart` has gained popularity over
  `tree`.

First we fit a general classification tree to the `Carseats` data set:
```{r fit_tree1}
# Create a variable for classification of 
# "low" and "high" sales based on sales cutoff of 8.0
# Append new categorical variable to the Carseats dataframe
# Ensure the response variable (High) is a factor for rpart::rpart
Carseats %<>% dplyr::mutate(High = ifelse(Sales <= 8, "No", "Yes") %>% factor(),
                            # `id` is for tracking samples below; train/test
                            id  = dplyr::row_number())

tree_carseats <- rpart::rpart(High ~ .,
                              data = dplyr::select(Carseats, -Sales, -id))
tree_carseats
printcp(tree_carseats)

# creates an empty plot
# to which we will add text of the tree
par(xpd = NA)                       # the default plot cuts off margins; enlarge
plot(tree_carseats, uniform = TRUE) # branch length equal; unrelated to gini
text(tree_carseats, use.n = TRUE, cex = 0.8)   # add text labels
title(main = "CART tree for High/Low sales of carseats")  # add title

# Just for fun -> include Sales as a predictor
# What do you think will happen?
tree_carseats_trivial <- rpart::rpart(High ~ ., Carseats)
tree_carseats_trivial

# creates an empty plot
plot(tree_carseats_trivial, uniform = TRUE)  # uniform vertical spacing of nodes
text(tree_carseats_trivial)
title(main = "CART tree for High/Low sales of carseats")
tree_carseats_trivial
```


-----------------



# Fitting Classification Tree Test Set
To properly fit a model, we must evaluate performance based on a hold-out 
test set:

```{r fit_tree2}
set.seed(2)                              # reproducible
train_car <- Carseats %>% 
  dplyr::sample_frac(size = 0.5)         # random selection of rows @ 50% = 200
test <- Carseats %>%  
  dplyr::anti_join(train_car, by = "id") %>%  # use anti_join to get the sample setdiff 
  dplyr::select(-Sales, -id)              # remove Sales and merge identifier
train_car %<>% dplyr::select(-Sales, -id)     # remove from training also

tree_carseats <- rpart::rpart(High ~ ., data = train_car)

# set up a `tibble` containing `truth` and `predictions`:
tree_pred     <- tibble::tibble(High = test$High,
                                pred = predict(tree_carseats,
                                               newdata = test,
                                               type = "class"))
tree_pred

# Get performance metrics via `yardstick`
tree_pred %>% yardstick::conf_mat(truth = High, estimate = pred)  # confusion
tree_pred %>% yardstick::accuracy(truth = High, estimate = pred)  # accuracy
tree_pred %>% yardstick::sens(truth = High, estimate = pred)      # sensitivity
tree_pred %>% yardstick::spec(truth = High, estimate = pred)      # specificity

```



------------------------------



## Cross-validated trees
We look at cross-validation to determine optimal level of tree complexity
based on cross-validated error, i.e., *misclassification*. Luckily, this has
been calculated for us "under-the-hood" within `rpart`; we simply need
to access it.

```{r cv, fig.width = 10, fig.height = 4}
rpart::printcp(tree_carseats)  # print cross-validation results
rpart::plotcp(tree_carseats)   # plot cross-validation results
title("Tree Size")

# which level of complexity minimizes cv error?
best_cp <- tree_carseats$cptable %>% 
  data.frame() %>%      # easier to work with than matrices
  dplyr::slice(which.min(.$xerror)) %>% # get the row where `xerror` is min
  dplyr::pull("CP")                     # pull out `CP` from that row
                 
best_cp
```





--------------------------




## Pruning Trees
It is often a good idea to *prune* the tree to avoid over-fitting, this is
achieved via the `rpart::prune` function given a *complexity parameter*, 
`cp` (calculated above). Note the difference between *relative error* (plotted) and *xerror* (which we are using).

```{r pruning, warning = FALSE}
# Prune to 8-node tree and predict again using new tree
# cp is the complexity parameter; 
#prune_carseats_8 <- rpart::prune(tree_carseats, cp = 0.0208)
prune_carseats_8 <- rpart::prune(tree_carseats, cp = best_cp)
par(xpd = NA)
plot(prune_carseats_8, uniform = TRUE)
text(prune_carseats_8, use.n = TRUE, cex = 0.8)
title(main = "8-leaf classification tree for carseats")

# Calculate performance metrics
tree_pred_8 <- predict(prune_carseats_8, test, type = "class")
tibble(test = test$High,
       pred = tree_pred_8) %>% 
  yardstick::conf_mat(truth = test, estimate = pred)  # confusion matrix
tibble(test = test$High,
       pred = tree_pred_8) %>% 
  yardstick::accuracy(truth = test, estimate = pred)  # accuracy
```



-----------------------------







# Regression Trees: `Boston` Data
We next turn our attention to a regression setting using the `Boston` data set.

```{r Regression_trees}
Boston %<>% tibble::as.tibble() %>% 
  dplyr::mutate(id = dplyr::row_number()) 
Boston

set.seed(1)                              # reproducible
train_boston <- Boston %>% 
  dplyr::sample_frac(size = 0.5)         # random selection of rows @ 50% = 200
test_boston <- Boston %>%  
  dplyr::anti_join(train_boston, by = "id") %>%  # use anti_join to get the sample setdiff 
  dplyr::select(-id)              # remove id 
train_boston %<>% dplyr::select(-id)     # remove id

boston_tree <- rpart::rpart(medv ~ ., data = train_boston)
boston_tree
rpart::printcp(boston_tree)

par(xpd = NA)
plot(boston_tree, uniform = TRUE)          # view the full tree
title(main = "Regression Tree for Median House Values")
text(boston_tree, use.n = TRUE, cex = 0.8)

par(mfrow = c(1, 2))
rpart::rsq.rpart(boston_tree)  # regression R^2 CV statistics

par(mfrow = c(1, 1))
rpart::plotcp(boston_tree)     # plot complexity across tree size as above
title("Tree Size")

# best size seems to be ~7-8
# we'll go with cp = 0.016
prune_boston <- rpart::prune(boston_tree, cp = 0.016)
par(xpd = NA)
plot(prune_boston, uniform = TRUE)
text(prune_boston, use.n = TRUE, cex = 0.8)
title(main = "7-leaf classification tree for carseats")

# Calculate performance metrics
yhat_1 <- predict(prune_boston, newdata = test_boston)
mse1  <- mean((yhat_1 - test_boston$medv)^2)
mse1

# Plotting predictions via `ggplot`
tibble::tibble(actual_medv    = test_boston$medv,
               predicted_medv = yhat_1) %>% 
  ggplot(aes(x = predicted_medv, y = actual_medv)) +
    geom_point(alpha = 0.5, position = position_jitter(width = 0.25)) +  # jitter in `x`
    geom_smooth(method = "lm") +                    # add linear fit with se CI95
    expand_limits(x = c(10, 50), y = c(10, 50)) +   # make square
    geom_abline(colour = "red", alpha = 0.5, linetype = "longdash") +  # unit line
    ggtitle("Regression tree prediction for median house values") +
    NULL
```





--------------------------------





# Bagging and Random Forests: `Boston` Data
Recall:

  * Bagging is a special case of random forest with `mtry = p`, where `p` is
  the number of predictors to try at each split.
  * For this example, `mtry = 13`

```{r random_forest}
set.seed(1)
rf_boston13 <- randomForest::randomForest(medv ~ ., data = train_boston,
                                          mtry = 13, importance = TRUE)
rf_boston13
yhat_2 <- predict(rf_boston13, newdata = dplyr::select(test_boston, -medv))
mse2   <- mean((yhat_2 - test_boston$medv)^2)
mse2


# Plotting predictions via `ggplot`
tibble::tibble(actual_medv    = test_boston$medv,
               predicted_medv = yhat_2) %>% 
  ggplot(aes(x = predicted_medv, y = actual_medv)) +
    geom_point(alpha = 0.5) +               # no jitter necessary this time
    geom_smooth(method = "lm") +                    # add linear fit with se CI95
    expand_limits(x = c(10, 50), y = c(10, 50)) +   # make square
    geom_abline(colour = "red", alpha = 0.5, linetype = "longdash") +  # unit line
    ggtitle("Random Forest with all variables at each split") +
    NULL

# Reduce total number of trees in the `ensemble` forest (default = 500)
rf_boston25 <- randomForest::randomForest(medv ~ ., data = train_boston,
                                          mtry = 13, ntree = 25)
yhat_3 <- predict(rf_boston25, newdata = dplyr::select(test_boston, -medv))
mse3   <- mean((yhat_3 - test_boston$medv)^2)
mse3

# Reduce the number of variables/predictors randomly 
# sampled at each split (`mtry = 6`)
set.seed(1)
rf_boston6 <- randomForest::randomForest(medv ~ ., data = train_boston,
                                         mtry = 6, importance = TRUE)
yhat_4 <- predict(rf_boston6, newdata = dplyr::select(test_boston, -medv))
mse4   <- mean((yhat_4 - test_boston$medv)^2)
mse4
```


## Gini Importance
  1. Mean decrease in accuracy in predictions on out-of-bag samples 
     when variable is excluded.
  2. Measure of the total decrease in node impurity that results from 
     splits over the variable -> Averaged over all trees in forest.
  3. See laboratory *followup_notes* file: `gini_backward_rf.nb.html`
     for additional discussion about `Gini` importance.

```{r gini}
importance(rf_boston6)       # built-in S3 print method for class `randomForest`
varImpPlot(rf_boston6)       # built-in S3 plot method for class `randomForest`
``` 



## Iterate using `purrr`
We can iterate over possible values of `mtry` and `ntree` to obtain a view
of how these parameters affect `mse`. We will do with with the `purrr` package
and plot with `ggplot2` all in one pipe chain.

```{r rf_optimization, fig.width = 10}
set.seed(101)
n_feat    <- ncol(train_boston) - 1 
mtry_vec  <- seq(floor(n_feat / 3), n_feat)
ntree_vec <- seq(50, 1000, by = 50)

mse_mtry <- purrr::map_dbl(mtry_vec, ~ {    # map_dbl -> output = double vector
  rf   <- randomForest::randomForest(medv ~ ., data = train_boston, mtry = .x)
  yhat <- predict(rf, newdata = dplyr::select(test_boston, -medv))
  mean((yhat - test_boston$medv)^2)
})

mse_ntree <- purrr::map_dbl(ntree_vec, ~ { 
  rf   <- randomForest::randomForest(medv ~ ., data = train_boston, ntree = .x)
  yhat <- predict(rf, newdata = dplyr::select(test_boston, -medv))
  mean((yhat - test_boston$medv)^2)
})

gg <- list()
gg$mtry <- tibble::tibble(mtry = mtry_vec,
                          MSE  = mse_mtry) %>% 
  ggplot(aes(x = mtry, y = MSE)) +
    geom_point(size = 3, colour = "blue") +
    geom_line(colour = "blue") +
    NULL
gg$ntree <- tibble::tibble(ntree = ntree_vec,
                           MSE   = mse_ntree) %>% 
  ggplot(aes(x = ntree, y = MSE)) +
    geom_point(size = 3, colour = "blue") +
    geom_line(colour = "blue") +
    NULL

gridExtra::grid.arrange(gg$mtry, gg$ntree, ncol = 2)   # plot in 1x2 grid
```



--------------------------------




# Boosting: `Boston` Data
Boosting is performed via the `gbm` package in `R`, which fits "Generalized
Boosted Regression Models". Recall that:

  * Argument `distribution = "gaussian"` fits regression models
  * Argument `distribution = "bernoulli"` fits classification models

```{r boosting}
set.seed(1)
gbm_boston1 <- gbm::gbm(medv ~ ., data = train_boston,
                        distribution = "gaussian",
                        n.trees = 5000, interaction.depth = 4)

# Relative influence
summary(gbm_boston1)
```


## GBM Plotting
```{r plotting, fig.width = 9, fig.height=5}
# Partial dependence plots illustrate the marginal effect 
# of the selected variables  on the response after integrating 
# out the other variables
par(mfrow = c(1, 2))
plot(gbm_boston1, i = "rm")     # built-in S3 plot method
title(main = "Marginal effect of \n number of rooms")

plot(gbm_boston1, i = "lstat")
title(main = "Marginal effect of \n % lower status")
```


## GBM Predicting & MSE
```{r predicting, warning = FALSE}
yhat_gbm1 <- predict(gbm_boston1, newdata = dplyr::select(test_boston, -medv),
                     n.trees = 5000)
mse5 <- mean((yhat_gbm1 - test_boston$medv)^2)
mse5

# Set shrinkage parameter: 0.001 -> 0.2 
gbm_boston0.2 <- gbm::gbm(medv ~ ., data = train_boston,
                          distribution = "gaussian", n.trees = 5000,
                          interaction.depth = 4, shrinkage = 0.2,
                          verbose = FALSE)
yhat.gbm0.2 <- predict(gbm_boston0.2,
                       newdata = dplyr::select(test_boston, -medv),
                       n.trees = 5000)
mse6 <- mean((yhat.gbm0.2 - test_boston$medv)^2)
mse6
```



---------------------------



# Appendix: CART via `caret`
The [caret](http://topepo.github.io/caret/index.html) package also provides 
a convenient framework for fitting cross-validated CART models in `R`. 

Two notes:

  * The `caret` package will "soon" be replaced by the now in development `parsnip` package
  * Cross-validation is *built-in* to the ensemble `random forest` algorithm

```{r caret, fig.height = 6, fig.width = 10, warning = FALSE}
ctrl <- caret::trainControl(
  method = "cv",
  classProbs = TRUE,    # predict the probabilities
  # Compute the ROC AUC as well as the sens and
  # spec from the default 50% cutoff. The 
  # function `twoClassSummary` produces those.
  summaryFunction = caret::twoClassSummary,
  savePredictions = "final",
  sampling = "down"  # for class imbalances; random down-sample prevelant class
)

set.seed(101)
cart_caret <- caret::train(
  x = dplyr::select(train_car, -High),  # no cheating!; rm 'truth' -> High
  y = train_car$High,                   # provide 'truth' as vector
  method = "rpart2",                # fit models via `rpart`
  metric = "ROC",                   # use ROC as cost function for performance
  tuneGrid = data.frame(maxdepth = 1:20),  # turning parameter; tree depth
  trControl = ctrl                  # use control object above as settings
)

cart_caret               # View the object with S3 print method
cart_caret$finalModel    # View the `best model`
caret_gg <- list()

# View CV ROCs by tuning par; S3 ggplot method
caret_gg$maxdepth <- ggplot(cart_caret) 

# Variable importance
cart_imp <- varImp(cart_caret, scale = FALSE, 
                   surrogates = FALSE, competes = FALSE)
caret_gg$imp <- ggplot(cart_imp, top = 7) + xlab("")
caret_gg %>% 
  purrr::invoke(gridExtra::grid.arrange, ., ncol = 2)
```


--------------------------

# Resources
[CART Methods by Joao Neto](http://www.di.fc.ul.pt/~jpn/r/tree/tree.html)


-----------------------------

Created on `r Sys.Date()` by the [Rmarkdown package](https://github.com/rstudio/rmarkdown) (v`r utils::packageVersion("rmarkdown")`) and `r R.version$version.string`.
