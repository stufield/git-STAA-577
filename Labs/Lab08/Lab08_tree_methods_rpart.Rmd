---
title: 'STAA 577: Laboratory Eight </br> Tree-based methods (`tidyverse`)'
author: 'Adapted by Tavener & Field </br> From: James, Witten, Hastie and Tibshirani'
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_notebook:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
ratio: '9:16'
tables: yes
fontsize: 12pt
---


### Load necessary libraries {-}
```{r setup, message = FALSE, warning = FALSE}
options(warnPartialMatchArgs = FALSE)  # don't want these warnings
library(magrittr)     # pipes
library(tibble)       # tibbles
library(dplyr)        # data wrangling
library(purrr)        # iteration
library(yardstick)    # classification performance measures
library(broom)        # summarizing models consistently
library(ggplot2)      # tidy plotting
library(ISLR)         # The `Carseats` data set
library(rpart)        # building classification trees
library(randomForest) # package name says it
library(MASS)         # The `Boston` data set
library(gbm)          # Boosting algorithms
library(caret)        # classification and Regression Training (CV)
library(gridExtra)    # arranging ggplot `grobs` into grid
```

-----------------------

# The `Carseats` Data Set
```{r Carseats}
Carseats %<>% tibble::as.tibble()
Carseats
dim(Carseats)
```


# Classification Trees

**NOTE**: In this lab we will use the `rpart::rpart()` function instead of 
the `tree::tree()` function used in the ISLR textbook uses, since
`rpart` has gained popularity over `tree` in the years since ISLR was 
published. `rpart` uses "recursive partitioning" to build its trees
(see "Classification and Regression Trees", by Breiman, Friedman, Olshen and Stone, 1984).
Experience has shown this results in a *slight* performance improvement for 
classification problems compared to `tree::tree()`, although performance is
similar in *this* data problem. 
See [rpart.pdf](https://cran.r-project.org/web/packages/rpart/rpart.pdf)


First we fit a general classification tree to the `Carseats` data set:

```{r fit_tree1}
# Create a variable for classification of "low" and "high" sales based on sales cutoff of 8.0
# Append new categorical variable to the Carseats dataframe
# Ensure the response variable (High) is a factor for rpart::rpart
Carseats %<>% dplyr::mutate(High = ifelse(Sales <= 8, "No", "Yes") %>% factor(),
                            # `id` is for tracking samples below; train/test
                            id  = dplyr::row_number())
Carseats
dim(Carseats)

tree_carseats <- rpart::rpart(High ~ .,
                              data = dplyr::select(Carseats, -Sales, -id))
tree_carseats
#printcp(tree_carseats)

# creates an empty plot to which we will add text of the tree
par(xpd = NA)                       # the default plot cuts off margins; enlarge
plot(tree_carseats, uniform = TRUE) # branch length equal; unrelated to gini
text(tree_carseats, use.n = TRUE, cex = 0.8)   # add text labels
title(main = "CART tree for High/Low sales of carseats")  # add title
```

```{r exercise, eval = FALSE}
# Exercise: Include Sales as a predictor. What do you think will happen?
tree_carseats_trivial <- rpart::rpart(High ~ ., Carseats)
tree_carseats_trivial
printcp(tree_carseats_trivial)
plot(tree_carseats_trivial, uniform = TRUE)  # uniform vertical spacing of nodes
text(tree_carseats_trivial)
title(main = "CART tree for High/Low sales of carseats")
```


-----------------


## Estimating the test error (by hand)
Evaluate performance based on a hold-out test set.

```{r fit_tree2}
set.seed(2)                                   # reproducible
train_car <- Carseats %>% 
  dplyr::sample_frac(size = 0.5)              # random selection of rows @ 50% = 200
test <- Carseats %>%
  dplyr::anti_join(train_car, by = "id") %>%  # use anti_join to get the sample setdiff
  dplyr::select(-Sales, -id)                  # remove Sales and merge identifier
train_car %<>% dplyr::select(-Sales, -id)     # remove from training also

tree_carseats <- rpart::rpart(High ~ ., data = train_car)

# set up a `tibble` containing `truth` and `predictions`:
tree_pred     <- tibble::tibble(High = test$High,
                                pred = predict(tree_carseats,
                                               newdata = test,
                                               type = "class"))
tree_pred

# Get performance metrics via `yardstick`
tree_pred %>% yardstick::conf_mat(truth = High, estimate = pred)  # confusion
tree_pred %>% yardstick::accuracy(truth = High, estimate = pred)  # accuracy
tree_pred %>% yardstick::sens(truth = High, estimate = pred)      # sensitivity
tree_pred %>% yardstick::spec(truth = High, estimate = pred)      # specificity

```


------------------------------

## Cross-validating trees

Cross-validation based on *misclassification* is performed "under-the-hood"
within `rpart`; we simply need to access it. 

Complexity parameter. Any split that does not decrease the overall lack of 
fit by a factor of `cp` is not attempted. For instance, with
anova splitting, this means that the overall R-squared must increase by `cp`
at each step. The main role of this parameter is to save computing time by
pruning off splits that are obviously not worthwhile. Essentially,the user
informs the program that any split which does not improve the fit by `cp`
will likely be pruned off by cross-validation, and that hence the program
need not pursue it.

Complexity penalties are actually ranges, not values. By default, the
geometric mean of each interval is used for cross validation.

```{r cv, fig.width = 10, fig.height = 4}
rpart::printcp(tree_carseats)  # print cross-validation results
rpart::plotcp(tree_carseats)   # plot cross-validation results
title("Tree Size")

# Determine the size of the complexity parameter which minimizes cv error
# tree_carseats$cptable
best_cp <- tree_carseats$cptable %>%
  data.frame() %>%                      # easier to work with than matrices
  dplyr::slice(which.min(.$xerror)) %>% # get the row where `xerror` is min
  dplyr::pull("CP")                     # pull out `CP` from that row

best_cp
```


--------------------------

## Pruning Trees

*Pruning*  trees to avoid over-fitting is achieved via the `rpart::prune()`
function given a *complexity parameter*, `cp` (calculated above).
Note the difference between *relative error* (plotted)
and *xerror* (which we are using).

### Choosing the best tree
```{r pruning, warning = FALSE}
# Prune to 9-node tree (8 splits) and predict again using new tree
#   cp is the complexity parameter;
#   prune_carseats_8 <- rpart::prune(tree_carseats, cp = 0.0208)
prune_carseats_9 <- rpart::prune(tree_carseats, cp = best_cp)
par(xpd = NA)
plot(prune_carseats_9, uniform = TRUE)
text(prune_carseats_9, use.n = TRUE, cex = 0.8)
title(main = "9-leaf classification tree for carseats")

# Calculate performance metrics
tree_pred_9 <- predict(prune_carseats_9, test, type = "class")
tibble(test = test$High,
       pred = tree_pred_9) %>%
  yardstick::conf_mat(truth = test, estimate = pred)  # confusion matrix
tibble(test = test$High,
       pred = tree_pred_9) %>%
  yardstick::accuracy(truth = test, estimate = pred)  # accuracy
```


### Choosing the number of nodes

```{r pruning2, warning = FALSE}
# Prune to 6-node tree (5 splits) and predict again using new tree
#   cp is the complexity parameter;
prune_carseats_6 <- rpart::prune(tree_carseats, cp = 0.03)
par(xpd = NA)
plot(prune_carseats_6, uniform = TRUE)
text(prune_carseats_6, use.n = TRUE, cex = 0.8)
title(main = "6-leaf classification tree for carseats")

# Calculate performance metrics
tree_pred_6 <- predict(prune_carseats_6, test, type = "class")
tibble(test = test$High,
       pred = tree_pred_6) %>%
  yardstick::conf_mat(truth = test, estimate = pred)  # confusion matrix
tibble(test = test$High,
       pred = tree_pred_6) %>%
  yardstick::accuracy(truth = test, estimate = pred)  # accuracy
```


-----------------------------

# Regression Trees
We next turn our attention to a regression setting using the `Boston` data set.

```{r Regression_trees}
Boston %<>% tibble::as.tibble() %>%
  dplyr::mutate(id = dplyr::row_number())
Boston

set.seed(1)                              # reproducible
train_boston <- Boston %>%
  dplyr::sample_frac(size = 0.5)         # random selection of rows @ 50% = 200
test_boston <- Boston %>%
  dplyr::anti_join(train_boston, by = "id") %>%  # use anti_join to get the sample setdiff
  dplyr::select(-id)                     # remove id
train_boston %<>% dplyr::select(-id)     # remove id

boston_tree <- rpart::rpart(medv ~ ., data = train_boston)
boston_tree
rpart::printcp(boston_tree)

par(xpd = NA)
plot(boston_tree, uniform = TRUE)          # view the full tree
title(main = "Regression Tree for Median House Values")
text(boston_tree, use.n = TRUE, cex = 0.8)
```


## Cross-validating trees 

```{r Regression_trees_cv}
par(mfrow = c(1, 2))
rpart::rsq.rpart(boston_tree)  # regression R^2 CV statistics
# Produces two plots. The first plots the r-square (apparent and apparent - 
# from cross-validation) versus the number of splits.
# The second plots the Relative Error(cross-validation) +/- 1-SE from
# cross-validation versus the number of splits
par(mfrow = c(1, 1))
rpart::plotcp(boston_tree)     # plot complexity across tree size as above
title("Tree Size")
# The cptable in the fit contains the mean and standard deviation of 
# the errors in the cross-validated prediction against each of the 
# geometric means, and these are plotted by this function.
# A good choice of cp for pruning is often the leftmost value for
# which the mean lies below the horizontal line
```


## Pruning trees 
```{r Regression_trees_optimal}
# best size seems to be ~7-8 -> we'll go with cp = 0.016
prune_boston <- rpart::prune(boston_tree, cp = 0.016)
par(xpd = NA)
plot(prune_boston, uniform = TRUE)
text(prune_boston, use.n = TRUE, cex = 0.8)
title(main = "7-leaf classification tree for carseats")

# Calculate performance metrics
yhat_1 <- predict(prune_boston, newdata = test_boston)
mse1  <- mean((yhat_1 - test_boston$medv)^2)
mse1

# Plotting predictions via `ggplot`
tibble::tibble(actual_medv    = test_boston$medv,
               predicted_medv = yhat_1) %>%
  ggplot(aes(x = predicted_medv, y = actual_medv)) +
    geom_point(alpha = 0.5, position = position_jitter(width = 0.25)) +  # jitter in `x`
    geom_smooth(method = "lm") +                    # add linear fit with se CI95
    expand_limits(x = c(10, 50), y = c(10, 50)) +   # make square
    geom_abline(colour = "red", alpha = 0.5, linetype = "longdash") +  # unit line
    ggtitle("Regression tree prediction for median house values") +
    NULL
```


--------------------------------

# Bagging
## Random Forest
Bagging is a special case of random forest with `mtry = p`, where `p` is
the number of predictors to try at each split. Here `p = 13` so for
this example `mtry = 13`.

```{r random_forest}
set.seed(1)
dim(train_boston)
rf_boston13 <- randomForest::randomForest(medv ~ ., data = train_boston,
                                          mtry = 13, importance = TRUE)
rf_boston13
yhat_2 <- predict(rf_boston13, newdata = dplyr::select(test_boston, -medv))
mse2   <- mean((yhat_2 - test_boston$medv)^2)
mse2


# Plotting predictions via `ggplot`
tibble::tibble(actual_medv    = test_boston$medv,
               predicted_medv = yhat_2) %>%
  ggplot(aes(x = predicted_medv, y = actual_medv)) +
    geom_point(alpha = 0.5) +               # no jitter necessary this time
    geom_smooth(method = "lm") +                    # add linear fit with se CI95
    expand_limits(x = c(10, 50), y = c(10, 50)) +   # make square
    geom_abline(colour = "red", alpha = 0.5, linetype = "longdash") +  # unit line
    ggtitle("Random Forest with all variables at each split") +
    NULL

# Reduce total number of trees in the `ensemble` forest (default = 500)
rf_boston25 <- randomForest::randomForest(medv ~ ., data = train_boston,
                                          mtry = 13, ntree = 25)
yhat_3 <- predict(rf_boston25, newdata = dplyr::select(test_boston, -medv))
mse3   <- mean((yhat_3 - test_boston$medv)^2)
mse3
```



``` {r Random Forest}
# Reduce the number of variables/predictors randomly
# sampled at each split (`mtry = 6`)
set.seed(1)
rf_boston6 <- randomForest::randomForest(medv ~ ., data = train_boston,
                                         mtry = 6, importance = TRUE)
yhat_4 <- predict(rf_boston6, newdata = dplyr::select(test_boston, -medv))
mse4   <- mean((yhat_4 - test_boston$medv)^2)
mse4
```


## Gini Importance
1. Mean decrease in accuracy of predictions on out-of-bag samples when 
   variable is excluded.
2. Measure of the total decrease in node impurity that results from splits
   over the variable when averaged over all trees in forest.
3. See laboratory *followup_notes* file: `gini_backward_rf.nb.html` for
   additional discussion about `Gini` importance.

```{r gini}
importance(rf_boston6)    # built-in S3 print method for class `randomForest`
varImpPlot(rf_boston6)    # built-in S3 plot method for class `randomForest`
```


## Optimizing `mtry` and `ntree`
We can iterate over possible values of `mtry` and `ntree` to obtain a view
of how these parameters affect `mse`. We will do with with the `purrr`
package and plot with `ggplot2` all in one pipe chain.

```{r rf_optimization, fig.width = 10}
set.seed(101)

# Establish values for mtry and ntree
n_feat    <- ncol(train_boston) - 1
mtry_vec  <- seq(floor(n_feat / 3), n_feat)
ntree_vec <- seq(50, 1000, by = 50)

mse_mtry <- purrr::map_dbl(mtry_vec, ~ {    # map_dbl -> output = double vector
  rf   <- randomForest::randomForest(medv ~ ., data = train_boston, mtry = .x)
  yhat <- predict(rf, newdata = dplyr::select(test_boston, -medv))
  mean((yhat - test_boston$medv)^2)
})

mse_ntree <- purrr::map_dbl(ntree_vec, ~ {
  rf   <- randomForest::randomForest(medv ~ ., data = train_boston, ntree = .x)
  yhat <- predict(rf, newdata = dplyr::select(test_boston, -medv))
  mean((yhat - test_boston$medv)^2)
})

gg <- list()
gg$mtry <- tibble::tibble(mtry = mtry_vec,
                          MSE  = mse_mtry) %>%
  ggplot(aes(x = mtry, y = MSE)) +
    geom_point(size = 3, colour = "blue") +
    geom_line(colour = "blue") +
    NULL
gg$ntree <- tibble::tibble(ntree = ntree_vec,
                           MSE   = mse_ntree) %>%
  ggplot(aes(x = ntree, y = MSE)) +
    geom_point(size = 3, colour = "blue") +
    geom_line(colour = "blue") +
    NULL

gridExtra::grid.arrange(gg$mtry, gg$ntree, ncol = 2)   # plot in 1x2 grid
```


--------------------------------

# Boosting
Boosting is performed via the `gbm` package in `R`, which fits "Generalized
Boosted Regression Models". Recall that:

  * Argument `distribution = "gaussian"` fits regression models
  * Argument `distribution = "bernoulli"` fits classification models

```{r boosting}
set.seed(1)
gbm_boston1 <- gbm::gbm(medv ~ ., data = train_boston,
                        distribution = "gaussian",
                        n.trees = 5000, interaction.depth = 4)

# Relative influence
summary(gbm_boston1)
```


```{r plotting, fig.width = 9, fig.height=5}
# Partial dependence plots illustrate the marginal effect of the
# selected variables  on the response after integrating out the other variables
par(mfrow = c(1, 2))
plot.gbm(gbm_boston1, i = "rm")     # built-in S3 plot method
title(main = "Marginal effect of \n number of rooms")
plot.gbm(gbm_boston1, i = "lstat")
title(main = "Marginal effect of \n % lower status")
```


## Choice of shrinkage parameter
```{r predicting, warning = FALSE}

# Shrinkage parameter = 0.001 (default)
yhat_gbm1 <- predict(gbm_boston1, newdata = dplyr::select(test_boston, -medv),
                     n.trees = 5000)
mse5 <- mean((yhat_gbm1 - test_boston$medv)^2)
mse5

# Shrinkage parameter = 0.2
gbm_boston0.2 <- gbm::gbm(medv ~ ., data = train_boston,
                          distribution = "gaussian", n.trees = 5000,
                          interaction.depth = 4, shrinkage = 0.2,
                          verbose = FALSE)
yhat.gbm0.2 <- predict(gbm_boston0.2,
                       newdata = dplyr::select(test_boston, -medv),
                       n.trees = 5000)
mse6 <- mean((yhat.gbm0.2 - test_boston$medv)^2)
mse6
```


---------------------------

# Appendix: CART via `caret`
The [caret](http://topepo.github.io/caret/index.html) package also provides
a convenient framework for fitting cross-validated CART models in `R`.

Two notes:

  * The `caret` package will "soon" be replaced by the now in
    development `parsnip` package
  * Cross-validation is *built-in* to the ensemble `random forest` algorithm

```{r caret, fig.height = 6, fig.width = 10, warning = FALSE}
ctrl <- caret::trainControl(
  method = "cv",
  classProbs = TRUE,    # predict the probabilities
  # Compute the ROC AUC as well as the sens and
  # spec from the default 50% cutoff. The
  # function `twoClassSummary` produces those.
  summaryFunction = caret::twoClassSummary,
  savePredictions = "final",
  sampling = "down"  # for class imbalances; random down-sample prevelant class
)

set.seed(101)
cart_caret <- caret::train(
  x = dplyr::select(train_car, -High),  # no cheating!; rm 'truth' -> High
  y = train_car$High,                   # provide 'truth' as vector
  method = "rpart2",                # fit models via `rpart`
  metric = "ROC",                   # use ROC as cost function for performance
  tuneGrid = data.frame(maxdepth = 1:20),  # turning parameter; tree depth
  trControl = ctrl                  # use control object above as settings
)

cart_caret               # View the object with S3 print method
cart_caret$finalModel    # View the `best model`
caret_gg <- list()

# View CV ROCs by tuning par; S3 ggplot method
caret_gg$maxdepth <- ggplot(cart_caret)

# Variable importance
cart_imp <- varImp(cart_caret, scale = FALSE,
                   surrogates = FALSE, competes = FALSE)
caret_gg$imp <- ggplot(cart_imp, top = 7) + xlab("")
caret_gg %>%
  purrr::invoke(gridExtra::grid.arrange, ., ncol = 2)
```


--------------------------

# Resources
[CART Methods by Joao Neto](http://www.di.fc.ul.pt/~jpn/r/tree/tree.html)



<!--
# Comments SJT 
## Major -- caused in large part by the use of rpart!!
* In do not fully understand rpart::rsq.rpart.
  Is the apparent and relative r-squared terminology common?
  + No, I actually don't understand it myself, but hoped you had covered it
    in class. I typically use relative Rsq. This seems like a weird oddity
    of rpart.
-->


-----------------------------

Created on `r Sys.Date()` by the [Rmarkdown package](https://github.com/rstudio/rmarkdown)
(v`r utils::packageVersion("rmarkdown")`) and `r R.version$version.string`.
