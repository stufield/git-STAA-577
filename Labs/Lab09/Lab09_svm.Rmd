---
title: 'STAA 577: Laboratory Nine </br> `tidyverse` version'
author: 'Adapted by Tavener & Field </br> From: James, Witten, Hastie and Tibshirani'
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_notebook:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
ratio: '9:16'
tables: yes
fontsize: 12pt
editor_options:
  chunk_output_type: inline
---



# Setup

```{r setup, message = FALSE, warning = FALSE}
options(warnPartialMatchArgs = FALSE)  # don't want these warnings
library(magrittr)     # pipes
library(tibble)       # tibbles
library(dplyr)        # data wrangling
library(purrr)        # iteration
library(yardstick)    # classification performance measures
library(ggplot2)      # tidy plotting
library(ISLR)         # The `Carseats` data set
library(e1071)        # package for SVMs (but also naive Bayes)
library(gridExtra)    # arranging ggplot `grobs` into grid
library(pROC)         # ROC curves
```




-------------------------




# Simulate Data


```{r sim_data}
set.seed(1)
#x <- matrix(rnorm(20 * 2), ncol = 2)
#print(x)
#y <- c(rep(-1, 10), rep(1, 10))
#print(y)

# Add 1 to x2 where class is positive one (as opposed to negative 1)
# Class +1 should have larger x2 than class -1
# Class +1 and class -1 should be indisinguishable in terms of x1
# SIMON: ARE YOU SURE ABOUT THIS? IT SEEMS TO BE ADDING +1 TO *ALL* CLASS +1; BOTH x1 & x2
#x[y==1, ] <- x[y==1, ] + 1
#print(x)
#plot(x, col = (3 - y))

sim_data <- matrix(rnorm(20 * 2), ncol = 2) %>%  #    20x2 matrix of random gaussian N~(0,1)
  data.frame() %>%                           # convert to data frame
  magrittr::set_names(c("F1", "F2")) %>%     # name 2 features F1 & F2
  dplyr::mutate(y = c(rep(-1, 10), rep(1, 10))) %>%   # add a class variable (-1, 1)
  dplyr::mutate(F1 = ifelse(y == 1, F1 + 1, F1),      # add +1 to F1 for class +1
                F2 = ifelse(y == 1, F2 + 1, F2),      # add +1 to F2 for class +1
                y  = factor(y)) %>%         # make classes factor for model building
  tibble::as.tibble()                       # convert to tibble
sim_data


sim_data %>%
  ggplot(aes(x = F2, y = F1, colour = y)) +   # flip axes to match SVM plot method; F2 on `x`
  geom_point(alpha = 0.5, size = 4) +
  scale_colour_manual(values = c("blue", "red")) +
  NULL
```


----------------------------



# Support Vector Classifier
## Fitting SVM: `cost = 10` (default)

```{r SVM_classifier1}
# Cost of constraint violation = 10
svmfit1 <- e1071::svm(y ~ ., data = sim_data, kernel = "linear",
                     cost = 10, scale = FALSE)

# Support vectors are plotted as crosses, others as circles
# One point is misclassified at (x1, x2) ~= (-1.2, 1)
plot(svmfit1, sim_data)      # built-in S3 plot method class `svm`

# The support vectors are listed by index
svmfit1$index
summary(svmfit1)
svmfit1$coefs
sum((svmfit1$coefs)^2)
1 / sum((svmfit1$coefs)^2)
```

### Challenge Exercise
The plots above and below were generated via the S3 plot method for objects
of class `svm` written in `base R` plotting. Create your own `S3 ggplot method`
for class `svm` that produces a similar plot using `ggplot` style graphics.
**Hint**: it should be called `ggplot.svm` and is invoked via a call like this:

```{r gg_s3, eval = FALSE}
ggplot(svmfit2)
```


-----------------------------


## Fitting SVM: `cost = 0.1`
```{r SVM_classifier2}
# Cost of constraints violation = 0.1
svmfit2 <- e1071::svm(y ~ ., data = sim_data, kernel = "linear",
                      cost = 0.1, scale = FALSE)
plot(svmfit2, sim_data)
summary(svmfit2)

svmfit2$index
svmfit2$SV      # SIMON: this was commented out; your call to keep or remove
svmfit2$coefs
sum((svmfit2$coefs)^2)
1 / sum((svmfit2$coefs)^2)
svmfit2$rho     # SIMON: this was commented out; your call to keep or remove
```



-----------------------



## Tuning the SVM

```{r tuning_svm}
# Tune support vector classifier to find best cost of constraints
set.seed(1)
tune_out <- e1071::tune(svm, y ~ ., data = sim_data, kernel = "linear",
                        ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_out)
# Save the best model
bestmod <- tune_out$best.model
summary(bestmod)
```


## Simulate Test Data & Compute Confusion Matrix

```{r testing}
sim_test <- matrix(rnorm(20 * 2), ncol = 2) %>%  #    20x2 matrix of random gaussian N~(0,1)
  data.frame() %>%                           # convert to data frame
  magrittr::set_names(c("F1", "F2")) %>%     # name 2 features F1 & F2
  dplyr::mutate(y = sample(c(-1, 1), 20,
                           replace = TRUE)) %>%  # add random class variable (-1, 1)
  dplyr::mutate(F1 = ifelse(y == 1, F1 + 1, F1),      # add +1 to F1 for class +1
                F2 = ifelse(y == 1, F2 + 1, F2),      # add +1 to F2 for class +1
                y  = factor(y)) %>%         # make classes factor for model building
  tibble::as.tibble()                       # convert to tibble
sim_test

# Predict using the `best model`
best_pred_test_df <- data.frame(
  true_class      = sim_test$y,
  predicted_class = predict(bestmod, newdata = sim_test)
)
best_pred_test_df %>%
  yardstick::conf_mat(truth = true_class, estimate = predicted_class)
best_pred_test_df %>%
  yardstick::accuracy(truth = true_class, estimate = predicted_class)

# Fit and test using cost of constraint violation = 0.01
svmfit3 <- e1071::svm(y ~ ., data = sim_data, kernel = "linear",
                      cost = 0.01, scale = FALSE)
constraint_pred_test_df <- data.frame(
  true_class      = sim_test$y,
  predicted_class = predict(svmfit3, newdata = sim_test)
)
constraint_pred_test_df %>%
  yardstick::conf_mat(truth = true_class, estimate = predicted_class)
constraint_pred_test_df %>%
  yardstick::accuracy(truth = true_class, estimate = predicted_class)
```



--------------------------



# Linearly Separable Data

```{r svm_linearly_separable_data_set}
# simulate again -> greater separation between classes
# such that they can be separated by the linear hyperplane
set.seed(1)
linsep_data <- matrix(rnorm(20 * 2), ncol = 2) %>%
  data.frame() %>%                           # convert to data frame
  magrittr::set_names(c("F1", "F2")) %>%     # name 2 features F1 & F2
  dplyr::mutate(y = c(rep(-1, 10), rep(1, 10))) %>%   # add a class variable (-1, 1)
  dplyr::mutate(F1 = ifelse(y == 1, F1 + 1.5, F1),  # add +1 to F1 for class +1
                F2 = ifelse(y == 1, F2 + 1.5, F2),  # add +1 to F2 for class +1
                y  = factor(y)) %>%         # make classes factor for model building
  tibble::as.tibble()                       # convert to tibble
linsep_data


linsep_data %>%
  ggplot(aes(x = F2, y = F1, colour = y)) +
  geom_point(alpha = 0.5, size = 4) +
  scale_colour_manual(values = c("blue", "red")) +
  NULL

# Fit with cost of constraint violation = 1e5 -> no training errors
# Three support vectors:
#    + some non support vectors very close to separating hyperplane
svmfit_c100000 <- e1071::svm(y ~ ., data = linsep_data,
                             kernel = "linear", cost = 1e05)
summary(svmfit_c100000)
plot(svmfit_c100000, linsep_data)

# Change the cost of constraint violation = 1
# Seven support vectors -> smaller variance?
svmfit_c1 <- e1071::svm(y ~ ., data = linsep_data,
                        kernel = "linear", cost = 1)
summary(svmfit_c1)
plot(svmfit_c1, linsep_data)
```





---------------------------



# The Support Vector Machine
Simulate more data, this time a 10x sized data set:

```{r sim_data2}
# Create data set
set.seed(1)
sim_data200 <- matrix(rnorm(200 * 2), ncol = 2) %>%
  data.frame() %>%                           # convert to data frame
  magrittr::set_names(c("F1", "F2")) %>%     # name 2 features F1 & F2
  dplyr::mutate(id = dplyr::row_number()) %>%    # add index for sample ids
  dplyr::mutate(y  = rep(c(1,2), c(150,50))) %>%   # add a class variable (1, 2)
  dplyr::mutate(F1 = dplyr::case_when(                    # For F1
                       id <= 100 ~ F1 + 2,               # 1:100 -> bump +2
                       (id > 100 & id <= 150) ~ F1 - 2, # 101:150 -> bump -2
                       TRUE ~ F1                          # 151:200 -> bump 0
                      ),
                F2 = dplyr::case_when(
                       id <= 100 ~ F2 + 2,
                       (id > 100 & id <= 150) ~ F2 - 2,
                       TRUE ~ F2
                      ),
                y  = factor(y)) %>%   # make classes factor for model building
  tibble::as.tibble() %>%             # convert to tibble
  identity()                          # similar trick as `NULL` in ggplots

sim_data200 %>%
  ggplot(aes(x = F2, y = F1, colour = y)) +
  geom_point(alpha = 0.5, size = 4) +
  scale_colour_manual(values = c("blue", "red")) +
  NULL
```





---------------------------





```{r SVM2}
# Set up training/test data using form from `Lab08`
svm_train <- sim_data200 %>%
  dplyr::sample_frac(0.5)

svm_test <- sim_data200 %>%
  dplyr::anti_join(svm_train, by = "id") %>%
  dplyr::select(-id)

svm_train %<>% dplyr::select(-id)

# Fit SVM with radial kernel and cost of constraint violation = 1
# Gamma = parameter defining kernel
svm_radial_c1 <- e1071::svm(y ~ ., data = svm_train, kernel = "radial",
                            gamma = 1, cost = 1)
plot(svm_radial_c1, svm_train)
summary(svm_radial_c1)

# Fit SVM with radial kernel and cost of constraint violation = 1E5
# Is this too much?
svm_radial_c100000 <- e1071::svm(y ~ ., data = svm_train,
                                 kernel = "radial", gamma = 1, cost = 1e5)
plot(svm_radial_c100000, svm_train)
summary(svm_radial_c100000)

# Tune cost of constraint violation and kernel parameter (gamma) simultaneously
set.seed(1)
tune_svm <- e1071::tune(svm, y ~ ., data = svm_train, kernel = "radial",
                        ranges = list(cost  = c(0.1, 1, 10, 100, 1000),
                                      gamma = c(0.5, 1, 2, 3, 4)))
summary(tune_svm)

tuned_svm_df <- data.frame(
  true_class      = svm_test$y,
  predicted_class = predict(tune_svm$best.model,
                            #newx = dplyr::select(svm_test, -y))
                            # note: `newx` != `newdata` !!!
                            # textbook may have an error here
                            newdata = dplyr::select(svm_test, -y))
  # There is no `newx` argument to `predict()`; you want `newdata`
  # THIS IS A DIABOLICAL EDGE CASE; but on that arises when using `predict`
)

tuned_svm_df %>%
  yardstick::conf_mat(truth = true_class, estimate = predicted_class)
tuned_svm_df %>%
  yardstick::accuracy(truth = true_class, estimate = predicted_class) -> acc
acc
```

Thus, approximately `r acc*100`% of the observations are correctly classified
by this SVM.



# ROC curves
Typically class predictions are generated for a given probability cutoff, typically
0.5. Receiver Operator Criterion (ROC) curves can be seen as representing the
performance of a classifier for *all possible cutoffs*. With *sensitivity* in
[0, 1] and *1-specificity* also in [0,1], the total plotting area is 1.0, and
the "area under the curve" (AUC) can be used as a performance metric. Below
we use the `pROC` package to generate a ROC curve for the optimal SVM, with
$\gamma = 2$ and `cost = 1`.

```{r roc, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 5}
# to obtain class probabilities rather than
# class predictions, use the decision.values = TRUE argument
svm_optimal <- e1071::svm(y ~ ., data = svm_train, kernel = "radial",
                          gamma = 1, cost = 1, decision.values = TRUE)

# These "probabilities" are actually ratios
train_prob <- predict(svm_optimal, newdata = svm_train,
                      decision.values = TRUE) %>%
  attributes() %>%                  # prob ratios are "hidden" in the attributes
  purrr::pluck("decision.values") %>%   # in a list element called `decision.values`
  as.numeric()                      # in a nx1 matrix; we need a vector

roc_train <- pROC::roc(response = svm_train$y,
                       predictor = train_prob,
                       levels = c("1", "2"))

pROC::auc(roc_train)               # AUC

# ROC curve via S3 plot method
roc_train %>%
  plot(col = "navy",
       main = "Training (blue)\n Test (red)",
       print.thres = c(0.3, 0.5, 0.8),  # 3 arbitrarily chosen cutoffs
       print.thres.cex = 0.8,           # size of the text
       legacy.axes = TRUE,              # traditionally 1 - spec if plotted
       print.thres.pattern = "cut = %.2f (Spec = %.2f, Sens = %.2f)")

test_prob <- predict(svm_optimal, newdata = svm_test,
                     decision.values = TRUE) %>%
  attributes() %>%
  purrr::pluck("decision.values") %>%
  as.numeric()

roc_test <- pROC::roc(response = svm_test$y,
                      predictor = test_prob,
                      levels = c("1", "2"))

pROC::auc(roc_test)               # AUC

roc_test %>%
  plot(col = "darkred",            # red is test ROC
       add = TRUE,                 # add to existing plot; navy = training
       legacy.axes = TRUE)
```




--------------------------


# SVM with Multiple Classes

```{r multiple_classes}
set.seed(1)
sim_data250 <- matrix(rnorm(50 * 2), ncol = 2) %>%
  data.frame() %>%                        # convert to data frame
  magrittr::set_names(c("F1", "F2")) %>%  # name 2 features F1 & F2
  dplyr::mutate(y  = rep(0, 50),          # create third class variable (0)
                F2 = F2 + 2) %>%          # add +2 to F2 only
  # now add newly simulated data to existing sim_data200
  # must first remove the `id` column
  rbind(dplyr::select(sim_data200, -id)) %>%  # strips factor levels from `y`; converts -> character
  dplyr::mutate(y = factor(y)) %>%  # must refactor in the new `0` class
  tibble::as.tibble()

sim_data250

# Plot and see the newly generated `0` class in purple
# Original data from sim_data200 remains same (blue/red dots)
sim_data250 %>%
  ggplot(aes(x = F2, y = F1, colour = y)) +
  geom_point(alpha = 0.5, size = 4) +
  scale_colour_manual(values = c("purple", "blue", "red")) +
  NULL

svm_3class <- e1071::svm(y ~ ., data = sim_data250, kernel = "radial",
                         cost = 10, gamma = 1)
summary(svm_3class)
plot(svm_3class, sim_data250)
```





----------------------------






# Appendix: ROC with `ggplot2`
```{r ggroc, eval = FALSE}
ggroc <- function(roc, showAUC = TRUE, interval = 0.2,
                  breaks = seq(0, 1, interval)) {
  if ( !inherits(roc, "roc") ) {
    simpleError("Please provide roc object from `pROC` package.")
  }
  plotx <- rev(roc$specificities)
  ploty <- rev(roc$sensitivities)

  ggplot(NULL, aes(x = plotx, y = ploty)) +
    geom_segment(aes(x = 0, y = 1, xend = 1,yend = 0), alpha = 0.5) +
    geom_step() +
    scale_x_reverse(name = "Specificity",limits = c(1, 0),
                    breaks = breaks, expand = c(0.001, 0.001)) +
    scale_y_continuous(name = "Sensitivity", limits = c(0, 1),
                       breaks = breaks, expand = c(0.001, 0.001)) +
    theme(axis.ticks = element_line(color = "grey80")) +
    coord_equal() +
    annotate("text", x = interval / 2, y = interval / 2, vjust = 0,
             label = paste("AUC =", sprintf("%.3f", roc$auc)))
}
```



-----------------------------

Created on `r Sys.Date()` by the [Rmarkdown package](https://github.com/rstudio/rmarkdown) (v`r utils::packageVersion("rmarkdown")`) and `r R.version$version.string`.
