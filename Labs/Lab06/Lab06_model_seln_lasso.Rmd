---
title: 'STAA 577: Laboratory Six </br> `tidyverse` version'
author: 'Adapted by Tavener & Field </br> From: James, Witten, Hastie and Tibshirani'
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_notebook: 
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
ratio: '9:16'
tables: yes
fontsize: 12pt
---

# Setup

```{r setup, message = FALSE}
options(warnPartialMatchArgs = FALSE)  # don't want these warnings
library(magrittr)     # pipes
library(tibble)       # tibbles
library(dplyr)        # data wrangling
library(boot)         # for `cv.glm`
library(purrr)        # iteration
library(ggplot2)      # tidy plotting
library(ISLR)         # Hitters data set
library(leaps)        # step-wise model selection
library(gridExtra)    # arranging ggplots into grids
library(glmnet)       # logistic regression models
```



-----------------------------



# The `Hitters` Data Set

```{r data}
# Explore data set
Hitters %<>% as.tibble()      # let's be tidy and convert to tibble
names(Hitters)                # what are the variables to work with?
Hitters                       # view the tibble; 322 observations
sum(is.na(Hitters$Salary))    # there are 59 observations without Salary info
Hitters %<>% na.omit()        # let's remove them
nrow(Hitters)                 # we now have 263 observations
sum(is.na(Hitters))           # and no more missing data for Salary
```



------------------------



# Subset Selection
The `leaps::regsubsets` function performs an exhaustive search for the best 
subsets of the variables in `x` for predicting `y` in a Linear 
Regression setting, using an efficient branch-and-bound algorithm. It chooses
the best subset based on RSS. See `?leaps::regsubsets`.

```{r subset_selection}
regfit_full <- leaps::regsubsets(Salary ~ ., Hitters)

# Best two-variable model contains 
# Hits and CRBI features
# Rather verbose print method; only look at `obj` entry (Simon; pls double check)
summary(regfit_full)$obj

regfit_full <- leaps::regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg_summary <- summary(regfit_full)
names(reg_summary)
reg_summary$obj
```

Now let's view the results, both as a `tibble` and graphically
with `ggplot2::qplot`:

```{r subset_selection2}
# View the model performance metrics for each size model
tibble::tibble(
  RSS = reg_summary$rss,
  `R-squared` = reg_summary$rsq,
  `Adj_R-squared` = reg_summary$adjr2,
  CP = reg_summary$cp,
  BIC = reg_summary$bic
)


# RSS
p_rss <- qplot(seq_along(reg_summary$rss), reg_summary$rss,
      xlab = "Number of Variables", ylab = "RSS") +
  geom_line()

# Adjusted R^2
pt_adjr2 <- which.max(reg_summary$adjr2)   # index of max feature
p_adjr2  <- qplot(seq_along(reg_summary$adjr2), reg_summary$adjr2,
                  xlab = "Number of Variables", ylab = "Adjusted RSq") +
  geom_line() + 
  geom_point(aes(x = pt_adjr2, y = reg_summary$adjr2[pt_adjr2]),
             colour = "red", size = 3)
# CP
pt_cp <- which.min(reg_summary$cp)   # index of min feature
p_cp  <- qplot(seq_along(reg_summary$cp), reg_summary$cp,
               xlab = "Number of Variables", ylab = "Cp") +
  geom_line() +
  geom_point(aes(x = pt_cp, y = reg_summary$cp[pt_cp]),
             colour = "red", size = 3)
  

pt_bic <- which.min(reg_summary$bic)
p_bic  <- qplot(seq_along(reg_summary$bic), reg_summary$bic,
                xlab = "Number of Variables", ylab = "BIC") +
  geom_line() +
  geom_point(aes(x = pt_bic, y = reg_summary$bic[pt_bic]),
             colour = "red", size = 3)

# All 4 plots stored in variable `reg_plots`
# Now plot them in a grid
gridExtra::grid.arrange(p_rss,
                        p_adjr2,
                        p_cp,
                        p_bic, ncol = 2)
```


The `leaps` package has a (rather ugly) built in S3 plot
method (sorry, no `ggplot`) for class `regsubsets` which can be 
used to display the selected variables for the best model for a 
given number of predictors, ranked according to $R^2$. See `?plot.regsubsets`.

```{r plot_subsets}
par(mfrow = c(1, 2))
par(mgp = c(2, 0.75, 0), mar = c(3,4,3,1))
plot(regfit_full, scale = "r2")
plot(regfit_full, scale = "adjr2")
plot(regfit_full, scale = "Cp")
plot(regfit_full, scale = "bic")
coef(regfit_full, 6)   # Look at top 6 features
coef(regfit_full, 10)  # Look at top 10 features
```



## Forward and Backward Step-wise Selection
```{r stepwise_seln}
regfit_fwd <- leaps::regsubsets(Salary ~ ., data = Hitters,
                                nvmax = 19, method = "forward")
summary(regfit_fwd)
#regfit.fwd[2]
#regfit.fwd[[2]]
regfit_bwd <- leaps::regsubsets(Salary ~ ., data = Hitters,
                                nvmax = 19, method = "backward")
summary(regfit_bwd)

# Compare the models selected by each approach
coef(regfit_full, 7)
coef(regfit_fwd, 7)
coef(regfit_bwd, 7)
```



## Choosing Among Models

```{r Choosing_models}
set.seed(1)
train_lgl <- sample(c(TRUE, FALSE), nrow(Hitters), replace = TRUE)
test_lgl  <- !train_lgl    # invert logical
head(train_lgl); head(test_lgl)
regfit_best <- leaps::regsubsets(Salary ~ . ,
                                 data = Hitters[train_lgl, ],
                                 nvmax = 19)
test_mat <- model.matrix(Salary ~ ., data = Hitters[test_lgl, ])
?model.matrix
as.tibble(test_mat)    # view as a `tibble` to save space
```

Let's try to figure out what's going on in the `purrr` 
iteration loop below by stepping through the first two iterations:

```{r dive_into_loop}
# iter 1
coefi <- coef(regfit_best, id = 1)
test_mat[, names(coefi)] %>% head()
coefi
test_mat[, names(coefi)]%*%coefi %>% head()

# iter 2
coefi <- coef(regfit_best, id = 2)
test_mat[, names(coefi)] %>% head()
coefi
test_mat[, names(coefi)]%*%coefi %>% head()
```


The `purrr` iteration that generates the test MSE for each size model:

```{r the_loop}
val_errors <- purrr::map_dbl(1:19, function(.i) {
  coefi <- coef(regfit_best, id = .i)
  pred  <- test_mat[, names(coefi)]%*%coefi
  mean((Hitters$Salary[test_lgl] - pred)^2)
})
  
val_errors
which.min(val_errors)    # the 'best' model is one containing 10 predictors
which.min(val_errors) %>% coef(regfit_best, .)   # with these coefficients
```




### Define a S3 `predict` method
There is no `predict` method for objects of class `regbubsets`, so we
will create our own to replicated the steps above and simplify things
moving forward. S3 methods are not complicated, they just define a
specific behavior for a given class object (first argument) they act on.


```{r defn}
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  #print(form)
  mat   <- stats::model.matrix(form, newdata)
  coefi <- stats::coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}
```

## Choosing Among Models: cross-validation

```{r cv}
regfit_cv <- leaps::regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(regfit_cv, 10)


# k-fold cross validation
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace = TRUE)
folds

cv_errors <- list()     # pre-allocate an empty list
# Loop required here for scoping reasons
# so that it plays nice with our predict method
# otherwise the `call` will not be correct
for (fold in 1:k) {
  fit <- leaps::regsubsets(Salary ~ .,
                           data = Hitters[folds != fold, ],
                           nvmax = 19)
  purrr::map_dbl(1:19, function(.i) {
    fold_data <- Hitters[folds == fold, ]     # subset only fold idx
    pred <- predict(fit, fold_data, id = .i)  # this is our `predict`!
    mean((fold_data$Salary - pred)^2)         # MSE
  }) -> cv_errors[[fold]]    # RHS assignment! Can be cleaner code-wise
}

# 19 values for each fold
# 1 MSE per variable
purrr::map_int(cv_errors, length)    # sanity check

# Take the mean across the folds
mean_cv_errors <- cv_errors %>%
  data.frame() %>%        # convert to df for the `apply` call
  apply(., 1, mean)       # folds are now the columns; mean across folds

mean_cv_errors

# Plot the MSE for all 19 variables (model sizes)
pt_cv <- which.min(mean_cv_errors)
ggplot2::qplot(seq_along(mean_cv_errors),
               mean_cv_errors, ylab = "MSE",
               xlab = "Number of Variables") +
  geom_line() +
  geom_point(aes(x = pt_cv, y = mean_cv_errors[pt_cv]),
                 size = 3, colour = "red")

# The 11 variable model has the lowest MSE
leaps::regsubsets(Salary ~ ., data = Hitters, nvmax = 19) %>%
  coef(pt_cv)
```



-----------------------------



# Shrinkage Methods

## Ridge Regression I
We redefine the `Hitters` data set to include only the model matrix `x`
containing features (predictors) and a separate `y` variable
for the `Response` (Salary).

```{r ridge_data}
x <- model.matrix(Salary ~ ., Hitters)[, -1]   # remove Intercept column
head(x, 10)
y <- Hitters$Salary  # Salary is the Response variable we're predicting
summary(y)           # S3 method for numerics
y %>% data.frame() %>%
  magrittr::set_names("Salary") %>%     # rename `y` for the plot
  ggplot(aes(x = Salary)) +
  geom_histogram(aes(y = ..density..), fill = 'gray', colour = 'black') +
  geom_density(alpha = 0.2, fill = "blue")
```


Now perform a sweep across the tuning parameter `lambda` using
the `glmnet::glmnet` function.

```{r ridge_regression}
# lambda = 10^{10} effectively reduces the model to the mean; upper extreme
# lambda = 10^{-2} is effectively linear regression; lower extreme
grid <- 10^seq(10, -2, length = 100)
ggplot2::qplot(x = seq_along(grid), y = grid,
               ylab = expression(lambda),
               xlab = "") +
  geom_line(colour = "blue")

# Note: 
# alpha is the elastic net parameter, alpha in [0,1]
# alpha = 0 selects ridge regression
# alpha = 1 selects lasso
ridge_model1 <- glmnet(x, y, alpha = 0, lambda = grid)

# Matrix of coefficients for each value of penalty parameter
dim(coef(ridge_model1))
class(ridge_model1)

# Select the 50th value of the penalty parameter lambda
# It is also the 50th value of `grid` we defined up front
ridge_model1$lambda[50]
coef(ridge_model1)[, 50]
sqrt(sum(coef(ridge_model1)[-1, 50]^2))
sqrt(sum(coef(ridge_model1)[2:20, 50]^2))

# Select the 60th value of the penalty parameter lambda
# This value should be smaller as lambda decreases
ridge_model1$lambda[60]
coef(ridge_model1)[, 60]
sqrt(sum(coef(ridge_model1)[-1, 60]^2))

# Find coefficients for penalty parameter (lambda) = 50; `s = `
predict(ridge_model1, s = 50, type = "coefficients")[1:20, ]

# x = 50 is bracketed by the 69th and 70th penalties
ridge_model1$lambda[69]
coef(ridge_model1)[, 69]
ridge_model1$lambda[70]
coef(ridge_model1)[, 70]
```


## Ridge Regression II
We now split the `Hitters` (now `x`) data set into a **training** and **test**
set to get a better estimate of how the model might perform on samples it
has not *"seen"* and/or trained on.

```{r ridge_regression2}
# Create training and test sets as above
set.seed(1)
n      <- nrow(x)            # number of total samples
train  <- sample(1:n, n/2)   # numeric indices; no replacement
test   <- -train             # negate with -ve, when indexing row is removed
y_test <- y[test]

# Estimating the test error via `glmnet`
ridge_model2 <- glmnet::glmnet(x[train, ], y[train],  # subset `x` into training set
                               alpha  = 0,            # do ridge-regression
                               lambda = grid,         # use same grid as above
                               thresh = 1e-12)
ridge_pred <- predict(ridge_model2,            # S3 glmnet predict function
                      s = 4,                   # set lambda = 4
                      newx = x[test, ])        # subset `x` into test set

mean((ridge_pred - y_test)^2)                  # Calculate test MSE

# Exploring limits: large lambda
# Predicting using the mean of the training data
mean((mean(y[train]) - y_test)^2)

# An extremely large penalty essentially reduces beta_1 to beta_p to be zero
# i.e., the prediction becomes the mean for all predictors
ridge_pred2 <- predict(ridge_model2, s = 1e10, newx = x[test, ])
mean((ridge_pred2 - y_test)^2)             # MSE


# Exploring limits: small lambda
ridge_pred3 <- predict(ridge_model2,
                       s = 0,             # lambda = 0; un-penalized; see `lm()`
                       newx = x[test, ])  # subset to test set
mean((ridge_pred3 - y_test)^2)            # calculate test MSE

# Compare results with simple linear model
# without penalty
stats::lm(y ~ x, subset = train)      # better output for un-penalized model

# Use type = 'coefficients' to return coeffs
# Note: very close approximation to the linear model with least squares
predict(ridge_model2, s = 0, type = "coefficients")[1:20, ]
```



## Ridge Regression III: cross-validation
It is better to use cross-validation to estimate *best* lambda. Luckily, there
is a build in function `glmnet::cv.glmnet` provided for this purpose.
By default, `K = 10` cross-validation is performed.

```{r ridge_regression3}
set.seed(1)
ridge_cv <- cv.glmnet(x[train, ],  # subset training `predictor matrix`
                      y[train],    # subset training `response vector`
                      alpha = 0)   # ridge on training set

# Plot MSE vs the lambda penalty sweep
plot(ridge_cv)            # S3 plot method for class `cv.glmnet`

# Store min lambda*
# Compare to the plot ... remember to log-transform!
lambda_star <- ridge_cv %>% purrr::pluck("lambda.min")
lambda_star

#
ridge_pred4 <- predict(ridge_model2,       # model fit on training set only
                       s = lambda_star,    # use lambda*
                       newx = x[test, ])   # predict on test set only
mean((ridge_pred4 - y_test)^2)             # calculate test MSE

# Now that we know lambda*, fit on *full* data set
full_fit_ridge <- glmnet::glmnet(x, y, alpha = 0)
predict(full_fit_ridge, type = "coefficients", s = lambda_star)[1:20, ]
```


-------------------------------


## The Lasso

```{r lasso}
lasso_model <- glmnet::glmnet(x[train, ],  # subset training `predictor matrix` 
                              y[train],    # subset training `response vector`
                              alpha = 1,       # now alpha = 1; lasso
                              lambda = grid)   # same lambda search grid

# Plot coefficients as a function of penalty parameter
# using built-in S3 plot method
plot(lasso_model)

# Once again, use k-fold cross validation (default K = 10)
set.seed(1)
lasso_cv <- glmnet::cv.glmnet(x[train, ], y[train], alpha = 1)

# Plot CV output
plot(lasso_cv)
lambda_star2 <- lasso_cv %>%
  purrr::pluck("lambda.min")
lasso_pred   <- predict(lasso_model,        # training set fitted model
                        s = lambda_star2,   # predict using lambda*
                        newx = x[test, ])   # predict on test set
mean((lasso_pred - y_test)^2)               # calculate test MSE

# Now that we know lambda*, fit on *full* data set
full_fit_lasso <- glmnet(x, y, alpha = 1, lambda = grid)
lasso_coeffs   <- predict(full_fit_lasso,        # glmnet fit on full data
                         type = "coefficients",  # return betas; not predictions
                         s = lambda_star2)[1:20, ]
lasso_coeffs

# Some Betas (coeffs) are zero as expected
# Remove them to get a better look at the model
lasso_coeffs %>%
  magrittr::extract(. != 0)
```
 
