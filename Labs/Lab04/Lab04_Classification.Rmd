---
title: 'STAA 577: Laboratory Four </br> Classification (`tidyverse`)'
author: 'Adapted by Tavener & Field </br> From: James, Witten, Hastie and Tibshirani'
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_notebook:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
ratio: '9:16'
tables: yes
fontsize: 12pt
---



-----------------------



### Load necessary libraries {-}
```{r setup, message = FALSE, warning = FALSE}
options(warnPartialMatchArgs = FALSE)  # don't want these warnings
library(magrittr)     # pipes
library(tibble)       # tibbles
library(dplyr)        # data wrangling
library(purrr)        # iteration
library(ggplot2)      # tidy plotting
library(broom)        # summarize model objects uniformly
library(yardstick)    # classification performance measures
library(class)        # knn()
library(MASS)         # lda() and qda()
library(ISLR)         # Smarket data set
```

-----------------------------

# S&P Stock Market (Smarket)
```{r Stock_Market_Data, error = TRUE}
class(Smarket)                           # currently data frame
Smarket %<>% as.tibble()                 # convert to `tibble` object
class(Smarket)                           # tibbles are `tidyverse` friendly
names(Smarket)                           # List features/covariates available
dim(Smarket)                             # dimensions of data
Smarket                                  # echo Smarket (top 10 rows)
summary(Smarket)                         # S3 summary method for class `data.frame`
pairs(Smarket)                           # pairs plot of 9-choose-2 combinations
# cor(Smarket)                           # Error; non-numerics in 'Direction'
class(Smarket$Direction)                 # Factor
cor(dplyr::select(Smarket, -Direction))  # remove 'Direction' and retry
Smarket %>%
  ggplot(aes(x = 1:length(Volume), y = Volume)) +   # plot `Volume` over time
  geom_point(alpha = 0.5) +                         # points
  xlab("Time") +
  geom_smooth()                                     # smooth fit
```


--------------------------------

# Logistic regression

## Using the entire data set
```{r Logistic_regression}
glm_fit <- stats::glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 +
                      Lag5 + Volume, data = Smarket, family = "binomial")
class(glm_fit)
summary(glm_fit)                  # Statistical summary
coef(glm_fit)                     # Coefficients of the fit
sum_tbl <- broom::tidy(glm_fit)   # use `broom` to summarize
sum_tbl
sum_tbl %>% dplyr::pull(p.value)  # `pull` vector of p-values for each covariate

# Make predictions (training only)
glm_probs <- predict(glm_fit, type = "response")    # S3 predict method
class(glm_probs)
glm_probs %>% head(10)
Smarket$Direction %>% head(10)
contrasts(Smarket$Direction)
glm_pred <- ifelse(glm_probs > 0.5, "Up", "Down")  # predict @ 0.5 cutoff

# Confusion matrix
cmat <- table(truth = Smarket$Direction, pred = glm_pred)
addmargins(cmat)

# Accuracy (training)
sum(diag(cmat)) / sum(cmat)

# Alternative method (sanity check)
# note: the mean of a boolean vector is the proportion of TRUE
# b/c TRUE = 1; FALSE = 0
mean(glm_pred == Smarket$Direction)  # proportion predicted correctly
```


### Confusion matrices
There are various (arbitrary!) ways to present prediction performance summaries,
none more `correct` than any other. For consistency, this course will use the
default construction below, where the `truth` standard is at left
and the predictions along the top. Negative predictions (i.e. `controls`) occur
first, followed by positive predictions. With this construction:

* `accuracy` is diagonal sum divided by the total sum
* `specificity` is determined by the proportion of correct *negative*
  predictions in the first row
* `sensitivity` is determined by the proportion of the correct *positive*
  predictions in the second row



```{r confusion, echo = FALSE}
c("TN", "FN", "FP", "TP") %>%
  matrix(ncol = 2, dimnames = list(Truth = c("neg", "pos"),
                                   Prediction = c("neg", "pos"))) %>%
  as.table()
```

There are other predictive measures (e.g. negative predictive value (NPV),
and PPV) that become more important depending on the problem. For a complete
guide see
[confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).

---------------------------




## Using a training (sub)set

```{r Logistic_regression2, error = TRUE}
# Training on data prior to 2005
train <- dplyr::filter(Smarket, Year < 2005)

# Test on data post-2004
test  <- dplyr::filter(Smarket, Year >= 2005)

# Sanity checks
nrow(train)
nrow(test)
nrow(train) + nrow(test) == nrow(Smarket)  # sanity check
head(test$Direction)

# Fit LR model to training set
# and immediately make predictions via `%>%`
glm_probs <- stats::glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                        data = train, family = "binomial")  %>%  # fit LR model
  predict(newdata = test, type = "response")     # predict on test set

# Recast predictions
cutoff   <- 0.5
glm_pred <- ifelse(glm_probs > cutoff, "Up", "Down")

# Confusion matrix
cmat <- table(truth = test$Direction, pred = glm_pred)
addmargins(cmat)

# Accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# Sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# Specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec

# Sanity checks
mean(glm_pred == test$Direction) == acc      # recall proportion trick!
mean(glm_pred != test$Direction) == 1 - acc  # machine precision error!

# Try `dplyr::near()`; safer than `==` when floating point errors can bite
dplyr::near(mean(glm_pred != test$Direction), 1 - acc)
```


---------------------------

## Using a subset of predictors
```{r Logistic_regression3}
# Fit using only 2 predictors; Lag1 and Lag2
glm_fit2 <- stats::glm(Direction ~ Lag1 + Lag2 ,
                       data = train,
                       family = "binomial")
glm_probs <- predict(glm_fit2, newdata = test, type = "response")
cutoff    <- 0.5
glm_pred  <- ifelse(glm_probs > cutoff, "Up", "Down")

# Confusion matrix
cmat <- table(truth = test$Direction, pred = glm_pred)
addmargins(cmat)

# Accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# Sanity check
mean(glm_pred == test$Direction) == acc

# Sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# Specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec

# Make predictions for specific values of Lag1 and Lag2 also possible
predict(glm_fit2,
        newdata = data.frame(Lag1 = c(1.2, 1.5),
                             Lag2 = c(1.1, -0.8)),
        type = "response")
```

----------------------


# Discriminant Analyses


## Linear Discriminant Analysis (LDA)

```{r LDA1, fig.height = 7}
lda_fit <- MASS::lda(Direction ~ Lag1 + Lag2, data = train)
class(lda_fit)
lda_fit
# plot histogram/density/both for the `training`(!) set; split by class labels
plot(lda_fit, type = "hist", col = "grey")     # S3 plot method
# Predict on the `test` set
# S3 predict method for class `lda` (above) returns 3 elements:
#   class     = the class prediction for each observation (prob. >= 0.5 cutoff)
#   x         = the linear discriminats
#   posterior = the posterior probabilities by class for each observation
lda_pred <- predict(lda_fit, newdata = test)   
head(lda_pred$x, 20)           # scores (linear discriminants) for each class
range(lda_pred$x)              # min/max score
```


**Sidenote:** To convince yourself that the `plot()` method above plots
the linear discriminants of the *training* set by default,
run the code below yourselves and compare:

```{r LDA2, eval = FALSE}
lda_pred_easy <- predict(lda_fit, newdata = train) # `predict()` on training set
lda_pred_easy %>%
  purrr::pluck("x") %>%            # pull out 'x'; linear discriminants
  hist(prob = TRUE, col = "grey",  # plot 'x' in histogram
       main = "", xlab = "Up/Down Combined")
```

```{r LDA3}
head(lda_pred$posterior, 20)   # posterior probabilities each class
head(lda_pred$class, 20)       # predicted class
lda_class <- lda_pred$class    # predicted class

# Confusion matrix
cmat <- table(truth = test$Direction, pred = lda_class)
addmargins(cmat)

# Accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# Sanity check
mean(lda_class == test$Direction) == acc

# Sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# Specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec

# Specify decision criteria
sum(lda_pred$posterior[, "Down"] >= 0.5)  # compare to addmargins() above
sum(lda_pred$posterior[, "Down"] < 0.5)   # compare to addmargins() above
sum(lda_pred$posterior[, "Down"] > 0.9)   # none are predicted at 0.9 or above
```


-------------------


## Quadratic Discriminant Analysis (QDA)
```{r QDA}
qda_fit <- MASS::qda(Direction ~ Lag1 + Lag2, data = train)
qda_fit
qda_class <- predict(qda_fit, newdata = test)$class

# Confusion matrix
cmat <- table(truth = test$Direction, pred = qda_class)
addmargins(cmat)

# Accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# Sanity check
mean(qda_class == test$Direction) == acc

# Sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# Specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec
```



-----------------------



# KNN: K-Nearest Neighbors

## K = 1
```{r KNN}
training_classes <- train$Direction  # KNN models pass true classes separately
knn_train    <- dplyr::select(train, Lag1, Lag2)  # and remove `Direction`
test_classes <- test$Direction
knn_test     <- dplyr::select(test, Lag1, Lag2)
set.seed(1)

# Smallest neighborhood, K = 1
knn_pred_class <- class::knn(train = knn_train,
                             test = knn_test,
                             cl = training_classes, k = 1)

# Confusion matrix
cmat <- table(truth = test_classes, pred = knn_pred_class)
addmargins(cmat)

# Accuracy: Really bad for K = 1!
acc <- sum(diag(cmat)) / sum(cmat)    # a coin flip!
acc

# Sanity check
mean(knn_pred_class == test_classes) == acc

# Sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# Specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec
```

## K = 1, 2, ..., 10

Iterate over `K = 1, 2, ..., 10` using `purrr::map_df()`.

```{r varyK}
purrr::map_df(1:10, function(.x) {
  cmat <- class::knn(knn_train, knn_test, training_classes, k = .x) %>%
    table(truth = test_classes, pred = .)
  tibble::tibble(K    = .x,
                 acc  = sum(diag(cmat)) / sum(cmat),     # accuracy
                 sens = prop.table(cmat["Up", ])[2],     # sensitivity
                 spec = prop.table(cmat["Down", ])[1])   # specificity
})
```


----------------------------

Created on `r Sys.Date()` by [Rmarkdown](https://github.com/rstudio/rmarkdown)
(v`r utils::packageVersion("rmarkdown")`) and `r R.version$version.string`.
