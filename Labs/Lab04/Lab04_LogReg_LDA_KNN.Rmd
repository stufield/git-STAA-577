---
title: 'STAA 577: Laboratory Four </br> `tidyverse` version'
author: 'Adapted by Tavener & Field </br> From: James, Witten, Hastie and Tibshirani'
date: "`r format(Sys.Date(), '%e %B %Y')`"
output:
  html_notebook:
    code_folding: show
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: no
ratio: '9:16'
tables: yes
fontsize: 12pt
---


# Setup

```{r setup, message = FALSE, warning = FALSE}
options(warnPartialMatchArgs = FALSE)  # don't want these warnings
library(magrittr)     # pipes
library(tibble)       # tibbles
library(dplyr)        # data wrangling
library(purrr)        # iteration
library(ggplot2)      # tidy plotting
library(broom)        # summarize model objects uniformly
library(class)        # KNN
library(MASS)         # qda()
library(ISLR)         # Smarket data set
```



-----------------------------



# The `Smarket` Data Set: S & P Stock Market
```{r Stock_Market_Data, error = TRUE}
class(Smarket)                           # currently data frame
Smarket %<>% as.tibble()                 # convert to `tibble` object
class(Smarket)                           # tibbles are `tidyverse` friendly
names(Smarket)                           # List features/covariates available
dim(Smarket)                             # dimensions of data
Smarket                                  # echo Smarket (top 10 rows)
summary(Smarket)                         # S3 summary method for class `data.frame`
pairs(Smarket)                           # pairs plot of 9-choose-2 combinations
cor(Smarket)                             # Error; non-numerics in 'Direction'
class(Smarket$Direction)                 # Factor
cor(dplyr::select(Smarket, -Direction))  # remove 'Direction' and retry
Smarket %>%
  ggplot(aes(x = 1:length(Volume), y = Volume)) +   # plot `Volume` over time
  geom_point(alpha = 0.5) +                         # points
  xlab("Time") +
  geom_smooth()                                     # smooth fit
```

--------------------------------


# Logistic Regression I
```{r Logistic_regression}
glm.fit <- stats::glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 +
                      Lag5 + Volume, data = Smarket, family = "binomial")
class(glm.fit)
summary(glm.fit)       # Coefficients for each feature
coef(glm.fit)          # Statistical summary for each feature
sum_tbl <- broom::tidy(glm.fit)   # use `broom` to easily summarize model objects
sum_tbl                           # take a look
sum_tbl %>% dplyr::pull(p.value)  # pull vector of P-values for each covariate

# make predictions (training only)
glm.probs <- predict(glm.fit, type = "response")    # S3 predict method
class(glm.probs)
glm.probs %>% head(10)
Smarket$Direction %>% head(10)
contrasts(Smarket$Direction)
glm.pred <- ifelse(glm.probs > 0.5, "Up", "Down")  # predict @ 0.5 cutoff

# confusion matrix
cmat <- table(truth = Smarket$Direction, pred = glm.pred)
addmargins(cmat)

# accuracy (training); diagonal of confusion matrix
sum(diag(cmat)) / sum(cmat)

# alternative method; sanity check
mean(glm.pred == Smarket$Direction)  # proportion predicted correctly
```


## A note about confusion matrices
There are various (arbitrary!) ways to present prediction performance summaries,
none more `correct` than any other. For consistency, this course will use the 
default construction below, where the `truth` standard is at left 
and the predictions along the top. Negative predictions (i.e. `controls`) occur
first, followed by positive predictions. With this construction `accuracy` is
diagonal sum divided by the total sum, `specificity` is determined
by the proportion of correct *negative* predictions in the first row, and 
`sensitivity` is determined by the proportion of the correct *positive* predictions
in the second row.
```{r confusion, echo = FALSE}
c("TN", "FN", "FP", "TP") %>%
  matrix(ncol = 2, dimnames = list(Truth = c("neg", "pos"),
                                   Prediction = c("neg", "pos"))) %>% 
  as.table()
```

  
---------------------------

# Logistic Regression II
```{r Logistic_regression2, error = TRUE}
# training on marker prior to 2005
train <- dplyr::filter(Smarket, Year < 2005)

# test on data post-2004
test  <- dplyr::filter(Smarket, Year >= 2005)

# perform sanity checks
nrow(train)
nrow(test)
nrow(train) + nrow(test) == nrow(Smarket)  # sanity check
head(test$Direction)

# fit LR model on training set
glm.probs <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                 data = train, family = "binomial")  %>%  # fit LR model
  predict(newdata = test, type = "response")     # predict on test set

# recast predictions
cutoff   <- 0.5
glm.pred <- ifelse(glm.probs > cutoff, "Up", "Down")

# confusion matrix
cmat <- table(truth = test$Direction, pred = glm.pred)
addmargins(cmat)

# accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec

# sanity checks
mean(glm.pred == test$Direction) == acc
mean(glm.pred != test$Direction) == 1 - acc  # machine precision error!

# try dplyr::near
dplyr::near(mean(glm.pred != test$Direction), 1 - acc)
```


---------------------------


# Logistic regression III
```{r Logistic_regression3}
# fit using only 2 predictors; Lag1 and Lag2
glm.fit <- glm(Direction ~ Lag1 + Lag2 ,
               data = train,
               family = "binomial")
glm.probs <- predict(glm.fit, newdata = test, type = "response")
cutoff    <- 0.5
glm.pred  <- ifelse(glm.probs > cutoff, "Up", "Down")

# confusion matrix
cmat <- table(truth = test$Direction, pred = glm.pred)
addmargins(cmat)

# accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# sanity check
mean(glm.pred == test$Direction) == acc

# sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec

# Make predictions of very specific values
# of Lag1 and Lag2 also possible
predict(glm.fit,
        newdata = data.frame(Lag1 = c(1.2, 1.5),
                             Lag2 = c(1.1, -0.8)),
        type = "response")
```
  
  
----------------------
  
  

# Linear Discriminant Analysis (LDA)
```{r LDA}
lda.fit <- MASS::lda(Direction ~ Lag1 + Lag2, data = train)
class(lda.fit)
lda.fit
plot(lda.fit)                  # S3 plot method; histogram
lda.pred <- predict(lda.fit, newdata = test)   # S3 predict method
head(lda.pred$posterior, 10)   # posterior probabilities each class
lda.class <- lda.pred$class    # predicted class

# confusion matrix
cmat <- table(truth = test$Direction, pred = lda.class)
addmargins(cmat)

# accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# sanity check
mean(lda.class == test$Direction) == acc

# sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec

sum(lda.pred$posterior[, "Down"] >= 0.5)  # compare to addmargins() above
sum(lda.pred$posterior[, "Down"] < 0.5)   # compare to addmargins() above
sum(lda.pred$posterior[, "Down"] > 0.9)   # none are predicted at 0.9 or above
```

-------------------


# Quadratic Discriminant Analysis (QDA)
```{r QDA}
qda.fit <- qda(Direction ~ Lag1 + Lag2, data = train)
qda.fit
qda.class <- predict(qda.fit, newdata = test)$class

# confusion matrix
cmat <- table(truth = test$Direction, pred = qda.class)
addmargins(cmat)

# accuracy
acc <- sum(diag(cmat)) / sum(cmat)
acc

# sanity check
mean(qda.class == test$Direction) == acc

# sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec
```


-----------------------


# KNN: K-Nearest Neighbors
```{r KNN}
training_classes <- train$Direction
train %<>% dplyr::select(Lag1, Lag2)
test_classes <- test$Direction
test  %<>% dplyr::select(Lag1, Lag2)
set.seed(1)

# very small neighborhood of k = 1
knn_pred_class <- class::knn(train, test, training_classes, k = 1)

# confusion matrix
cmat <- table(truth = test_classes, pred = knn_pred_class)
addmargins(cmat)

# accuracy: Really Bad for K = 1!
acc <- sum(diag(cmat)) / sum(cmat)    # coin flip!
acc

# sanity check
mean(knn_pred_class == test_classes) == acc

# sensitivity (predicting Up)
sens <- prop.table(cmat["Up", ])[2]
sens

# specificity (predicting Down)
spec <- prop.table(cmat["Down", ])[1]
spec
```


## Let's retry with varying neighborhood size.
We will iterate over `1:10` for values of *k* using `purrr::map_df`.
```{r varyK}
purrr::map_df(1:10, function(x) {
  cmat <- class::knn(train, test, training_classes, k = x) %>%
    table(truth = test_classes, pred = .)
  data.frame(K    = x,
             acc  = sum(diag(cmat)) / sum(cmat),
             sens = prop.table(cmat["Up", ])[2],
             spec = prop.table(cmat["Down", ])[1])
})
```



----------------------------

Created on `r Sys.Date()` by the [Rmarkdown package](https://github.com/rstudio/rmarkdown) (v`r utils::packageVersion("rmarkdown")`) and `r R.version$version.string`.
